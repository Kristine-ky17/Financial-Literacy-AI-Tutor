{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Mzgp0i2vAwPC",
        "outputId": "7090f411-e71b-428f-9614-9062b177fa44"
      },
      "outputs": [],
      "source": [
        "!pip install openai pandas python-dotenv\n",
        "!pip install xgboost lightgbm\n",
        "!pip install optuna\n",
        "!pip install optuna-integration[lightgbm]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UUFPEEWIOM8"
      },
      "outputs": [],
      "source": [
        "# --- Import libraries ---\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import datetime\n",
        "import logging\n",
        "from google.colab import drive\n",
        "\n",
        "# Hyperparameter tuning\n",
        "import optuna\n",
        "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
        "from optuna.integration import LightGBMPruningCallback\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBWfWJYM7DWY",
        "outputId": "56282c9c-4452-4631-cef2-8f218aead983"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')  # follow the prompt to authorize\n",
        "\n",
        "# Global variable to hold the generated DataFrame\n",
        "GLOBAL_DF = None\n",
        "\n",
        "add_more_data = False # This will increase runtime by adding more data\n",
        "\n",
        "# Filepath containing data\n",
        "FILE_PATH = '/content/drive/MyDrive/fintechSoc_synthetic_data.csv'\n",
        "\n",
        "# --- Load environment variables from .env file ---\n",
        "load_dotenv()\n",
        "\n",
        "# Access the OpenAI API key from the environment variable or use the provided key\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\", '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')\n",
        "\n",
        "if api_key is None:\n",
        "    raise ValueError(\"API key not found. Make sure OPENAI_API_KEY is defined in your .env file.\")\n",
        "\n",
        "# Initialize OpenAI client (adjust according to the API version if needed)\n",
        "client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "# Set up logging configuration\n",
        "logging.basicConfig(\n",
        "    filename='synthetic_data_generation.log',\n",
        "    filemode='a',\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    level=logging.INFO\n",
        ")\n",
        "\n",
        "logging.info(\"Started synthetic data generation script\")\n",
        "print(\"Started synthetic data generation script\")\n",
        "\n",
        "MODEL = 'gpt-3.5-turbo'\n",
        "\n",
        "# Define stages and their respective prompts\n",
        "stage_prompts = {\n",
        "    0: \"Explain a complex financial concept to a beginner.\",\n",
        "    1: \"Provide a quiz question related to investments.\",\n",
        "    2: \"Describe risk diversification in a simple way.\",\n",
        "    3: \"Encourage students to ask financial questions.\",\n",
        "    4: \"Adjust the lesson plan based on the student's understanding.\",\n",
        "    5: (\"Previous topics: Explain a complex financial concept to a beginner, \"\n",
        "         \"Provide a quiz question related to investments, \"\n",
        "         \"Describe risk diversification in a simple way, \"\n",
        "         \"Encourage students to ask financial questions, \"\n",
        "         \"Adjust the lesson plan based on the student's understanding. \"\n",
        "         \"Based on the previous questions, come up with a different and similar financial topic.\")\n",
        "}\n",
        "\n",
        "# Number of samples to generate for each stage\n",
        "samples_per_stage = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZJwkqNfCXQu",
        "outputId": "cea5b651-b93e-4bcd-b0a5-d0e9bde235b7"
      },
      "outputs": [],
      "source": [
        "if os.path.isfile(FILE_PATH):\n",
        "    print(\"Found existing CSV – loading it …\")\n",
        "    GLOBAL_DF = pd.read_csv(FILE_PATH)\n",
        "    print(f\"loaded successfully, with {len(GLOBAL_DF)} rows of data.\")\n",
        "else:\n",
        "    print(\"No existing CSV – starting fresh.\")\n",
        "    GLOBAL_DF = pd.DataFrame()               # empty placeholder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Piu0ZkcY-Akd"
      },
      "source": [
        "## Data Preprocessing: Logic for generating new data (normally only ran when data is empty)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Q8EcFM8oIRC4"
      },
      "outputs": [],
      "source": [
        "def generate_synthetic_data():\n",
        "    \"\"\"\n",
        "    Generates one response per (stage, sample) pair and appends each row\n",
        "    directly to the global DataFrame GLOBAL_DF.\n",
        "    \"\"\"\n",
        "    global GLOBAL_DF\n",
        "\n",
        "    # Ensure GLOBAL_DF exists with the correct columns\n",
        "    if GLOBAL_DF is None:\n",
        "        GLOBAL_DF = pd.DataFrame(\n",
        "            columns=[\n",
        "                \"stage\", \"sample\", \"prompt\",\n",
        "                \"generated_response\", \"model_used\", \"timestamp\"\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    # Generate data and append row‑by‑row\n",
        "    for stage, prompt in stage_prompts.items():\n",
        "        for sample in range(samples_per_stage):\n",
        "            logging.info(f\"Generating sample {sample+1} for Stage {stage}…\")\n",
        "            print(f\"Generating sample {sample+1} for Stage {stage}…\")\n",
        "\n",
        "            try:\n",
        "                response = client.chat.completions.create(\n",
        "                    model=MODEL,\n",
        "                    messages=[{\"role\": \"system\", \"content\": prompt}],\n",
        "                )\n",
        "                chatgpt_output = response.choices[0].message.content\n",
        "\n",
        "                # Build a one‑row DataFrame and concat it\n",
        "                new_row = pd.DataFrame([{\n",
        "                    \"stage\": stage,\n",
        "                    \"sample\": sample + 1,\n",
        "                    \"prompt\": prompt,\n",
        "                    \"generated_response\": chatgpt_output,\n",
        "                    \"model_used\": MODEL,\n",
        "                    \"timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                }])\n",
        "\n",
        "                GLOBAL_DF = pd.concat(\n",
        "                    [GLOBAL_DF, new_row],\n",
        "                    ignore_index=True\n",
        "                )\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error generating sample {sample+1} for stage {stage}: {e}\")\n",
        "                print(f\"Error generating sample {sample+1} for stage {stage}: {e}\")\n",
        "\n",
        "    logging.info(\"Data generation complete — GLOBAL_DF updated in place.\")\n",
        "    print(\"Data generation complete — GLOBAL_DF updated in place.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DY_UMkYV-1ac"
      },
      "source": [
        "## Data Saving: Generates new data, only if global_DF is empty. But remove the if condition if you would like to generate more data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WFjXjQHI-oKA",
        "outputId": "2b76ee1b-9523-4bfa-9539-05fd6d0b8184"
      },
      "outputs": [],
      "source": [
        "# Run the data generation function if the data hasn't been generated yet\n",
        "if GLOBAL_DF is None or GLOBAL_DF.empty or add_more_data:\n",
        "    print(\"Generating fresh synthetic data…\")\n",
        "\n",
        "    # Instead of saving to CSV, store the DataFrame in a global variable\n",
        "    generate_synthetic_data()\n",
        "\n",
        "    # Saves data that is generated\n",
        "    GLOBAL_DF.to_csv(\n",
        "        FILE_PATH,\n",
        "        mode='w',                # overwrite with full DF once (simpler & safe)\n",
        "        index=False,\n",
        "        header=True              # always keep header when you overwrite\n",
        "    )\n",
        "    print(f\"Successfully saved to {FILE_PATH} a total of {len(GLOBAL_DF)} rows of data\")\n",
        "else:\n",
        "    print(f\"GLOBAL_DF has {len(GLOBAL_DF)} rows – skipping generation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW42JiIL-_By"
      },
      "source": [
        "## Main logic of code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0c6DoEdNbjy"
      },
      "outputs": [],
      "source": [
        "df = GLOBAL_DF  # Use the global dataframe\n",
        "\n",
        "# --- Feature and Label extraction ---\n",
        "X_text = df[\"generated_response\"]\n",
        "y = df[\"stage\"]\n",
        "\n",
        "# --- Convert text to numerical features using TF-IDF ---\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X = vectorizer.fit_transform(X_text)\n",
        "\n",
        "# --- Train/Test split ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxbq7tD5vC8C"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_models(hyperparams_xgb=None, hyperparams_lgb=None):\n",
        "    \"\"\"\n",
        "    Trains and evaluates XGBoost and LightGBM models using the global dataframe GLOBAL_DF.\n",
        "\n",
        "    Parameters:\n",
        "        hyperparams_xgb (dict): Hyperparameters for the XGBoost model.\n",
        "        hyperparams_lgb (dict): Hyperparameters for the LightGBM model.\n",
        "\n",
        "    Returns:\n",
        "        dict: Evaluation metrics for both models.\n",
        "    \"\"\"\n",
        "    # --- Set default hyperparameters if none are provided ---\n",
        "    if hyperparams_xgb is None:\n",
        "        hyperparams_xgb = {\"use_label_encoder\": False, \"eval_metric\": \"mlogloss\", \"random_state\": 42}\n",
        "    if hyperparams_lgb is None:\n",
        "        hyperparams_lgb = {\"random_state\": 42}\n",
        "\n",
        "    # --- Train XGBoost ---\n",
        "    xgb_model = xgb.XGBClassifier(**hyperparams_xgb)\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "    xgb_preds = xgb_model.predict(X_test)\n",
        "\n",
        "    # --- Train LightGBM ---\n",
        "    lgb_model = lgb.LGBMClassifier(**hyperparams_lgb)\n",
        "    lgb_model.fit(X_train, y_train)\n",
        "    lgb_preds = lgb_model.predict(X_test)\n",
        "\n",
        "    # --- Evaluate ---\n",
        "    xgb_accuracy = accuracy_score(y_test, xgb_preds)\n",
        "    lgb_accuracy = accuracy_score(y_test, lgb_preds)\n",
        "    xgb_report = classification_report(y_test, xgb_preds, output_dict=True)\n",
        "    lgb_report = classification_report(y_test, lgb_preds, output_dict=True)\n",
        "\n",
        "    # --- Optional: Plot top 10 TF-IDF feature importances ---\n",
        "    def plot_feature_importance(model, model_name):\n",
        "        importances = model.feature_importances_\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "        feat_df = pd.DataFrame({\n",
        "            \"feature\": feature_names,\n",
        "            \"importance\": importances\n",
        "        }).sort_values(by=\"importance\", ascending=False).head(10)\n",
        "\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.barh(feat_df[\"feature\"], feat_df[\"importance\"])\n",
        "        plt.xlabel(\"Importance\")\n",
        "        plt.title(f\"Top 10 Features - {model_name}\")\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Plot feature importances for both models\n",
        "    plot_feature_importance(xgb_model, \"XGBoost\")\n",
        "    plot_feature_importance(lgb_model, \"LightGBM\")\n",
        "\n",
        "    # --- Print Evaluation Metrics ---\n",
        "    print(\"\\n=== XGBoost Evaluation ===\")\n",
        "    print(\"Accuracy: {:.4f}\".format(xgb_accuracy))\n",
        "    print(classification_report(y_test, xgb_preds))\n",
        "\n",
        "    print(\"\\n=== LightGBM Evaluation ===\")\n",
        "    print(\"Accuracy: {:.4f}\".format(lgb_accuracy))\n",
        "    print(classification_report(y_test, lgb_preds))\n",
        "\n",
        "    metrics = {\n",
        "        \"XGBoost\": {\"accuracy\": xgb_accuracy, \"report\": xgb_report},\n",
        "        \"LightGBM\": {\"accuracy\": lgb_accuracy, \"report\": lgb_report},\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def compare_models(hyperparams_xgb=None, hyperparams_lgb=None):\n",
        "    \"\"\"\n",
        "    Compares the performance of the XGBoost and LightGBM models using metrics\n",
        "    from train_and_evaluate_models.\n",
        "\n",
        "    Parameters:\n",
        "        hyperparams_xgb (dict): Hyperparameters for the XGBoost model.\n",
        "        hyperparams_lgb (dict): Hyperparameters for the LightGBM model.\n",
        "\n",
        "    Returns:\n",
        "        dict: Evaluation metrics for both models.\n",
        "    \"\"\"\n",
        "    metrics = train_and_evaluate_models(hyperparams_xgb, hyperparams_lgb)\n",
        "\n",
        "    xgb_accuracy = metrics[\"XGBoost\"][\"accuracy\"]\n",
        "    lgb_accuracy = metrics[\"LightGBM\"][\"accuracy\"]\n",
        "\n",
        "    print(\"\\n=== Model Comparison ===\")\n",
        "    print(\"XGBoost Accuracy: {:.4f}\".format(xgb_accuracy))\n",
        "    print(\"LightGBM Accuracy: {:.4f}\".format(lgb_accuracy))\n",
        "\n",
        "    if xgb_accuracy > lgb_accuracy:\n",
        "        print(\"XGBoost performs better based on accuracy.\")\n",
        "    elif lgb_accuracy > xgb_accuracy:\n",
        "        print(\"LightGBM performs better based on accuracy.\")\n",
        "    else:\n",
        "        print(\"Both models perform equally based on accuracy.\")\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "LrNtD5qxN5kO",
        "outputId": "874c70f0-09e3-4e7b-b2a1-8627f2acfb92"
      },
      "outputs": [],
      "source": [
        "# Run the comparison with default hyperparameters\n",
        "compare_models()\n",
        "\n",
        "# Alternatively, to experiment with different hyperparameters, you can call:\n",
        "# xgb_params = {\"use_label_encoder\": False, \"eval_metric\": \"mlogloss\", \"random_state\": 42, \"max_depth\": 5}\n",
        "# lgb_params = {\"random_state\": 42, \"num_leaves\": 31}\n",
        "# compare_models(xgb_params, lgb_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc5rljYDuUep"
      },
      "source": [
        "## Below are the functions used for hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjMtYFkssFeU"
      },
      "outputs": [],
      "source": [
        "def objective_lgb(trial):\n",
        "    # 1) Suggest hyperparameter values\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 16),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
        "    }\n",
        "\n",
        "    # 2) Train/validation split\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # 3) Add a pruning callback so unpromising trials stop early\n",
        "    pruning_cb = LightGBMPruningCallback(trial, \"binary_logloss\")\n",
        "\n",
        "    # 4) Fit model\n",
        "    model = LGBMClassifier(**params, random_state=42)\n",
        "\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        eval_metric='binary_logloss',\n",
        "        # use callbacks for both early stopping and verbosity\n",
        "        callbacks=[\n",
        "            early_stopping(stopping_rounds=30),\n",
        "            log_evaluation(period=10)        # logs eval metric every 10 rounds\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # 5) Return validation AUC\n",
        "    preds = model.predict_proba(X_val)\n",
        "\n",
        "    return roc_auc_score(\n",
        "        y_val,\n",
        "        preds,\n",
        "        multi_class='ovr',    # or 'ovo'\n",
        "        average='macro'       # or 'weighted'\n",
        "    )\n",
        "\n",
        "\n",
        "    # preds = model.predict_proba(X_val)[:, 1]\n",
        "    # return roc_auc_score(y_val, preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFEvU6POsNNn"
      },
      "outputs": [],
      "source": [
        "def objective_xgb(trial):\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 16),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
        "        'tree_method': 'hist',   # fast histogram-based split\n",
        "        'random_state': 42\n",
        "    }\n",
        "\n",
        "    clf = XGBClassifier(**params)\n",
        "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    # maximize AUC\n",
        "    scores = cross_val_score(clf, X, y, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
        "    return scores.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YWbM18PwuSA8",
        "outputId": "b165aa05-782a-47b4-9fb6-b08da2ea2aac"
      },
      "outputs": [],
      "source": [
        "# Choose direction based on your metric\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",                 # or \"minimize\"\n",
        "    sampler=optuna.samplers.TPESampler(), # efficient Bayesian search\n",
        "    pruner=optuna.pruners.MedianPruner()  # stops bad trials early\n",
        ")\n",
        "study.optimize(objective_lgb, n_trials=50, timeout=3600)\n",
        "\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(\"  Value: \", study.best_trial.value)\n",
        "print(\"  Params: \")\n",
        "for key, val in study.best_trial.params.items():\n",
        "    print(f\"    {key}: {val}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
